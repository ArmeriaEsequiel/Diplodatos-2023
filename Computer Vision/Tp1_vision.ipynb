{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT1WjbFzVFnB"
      },
      "source": [
        "# Breve introducción a ANN\n",
        "\n",
        "Tutorial basado en el artículo [Simple Introduction to Convolutional Neural Networks](https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac).\n",
        "\n",
        "Antes de comenzar con el alboratorio vamos a revisar los conceptos básicos de las *redes neuronales artificiales* (ANN por sus siglas en inglés) y como se aplican al procesamiento y análisis de imágenes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPCMJYVcgzm-"
      },
      "source": [
        "## Motivación: análisis de imagen\n",
        "\n",
        "En las unidades anteriores hemos estado hablando básicamente de detectar características visuales en imágenes. Los investigadores desarrollaron múltiples técnicas de visión computacional para tratar estos problemas: SIFT, FAST, SURF, BRIEF, ORB, KAZE, etc. Sin embargo, en general estas técnicas presentaban dificultades para ciertos problemas: los detectores son demasiado simples o están diseñados para problemas muy específicos.\n",
        "\n",
        "Entonces la pregunta que surgió fue bastante obvia: ¿qué pasa si aprendemos las características visual que queremos detectar? Y la respuesta es simple: necesitamos un sistema que pueda hacer *aprendizaje de características* (o Feature Learning).\n",
        "\n",
        "El aprendizaje de características es una técnica que permite que un sistema encuentre automáticamente características relevantes para una tarea determinada, de manera que permite reemplazar el diseño manual de características. **Redes neuronales artificiales** es una de las técnicas que permite hacer esto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofOzmZziZ8sH"
      },
      "source": [
        "## Redes Neuronales artificiales\n",
        "\n",
        "Una **red neural artificial** es un modelo computacional que se inspira en la forma en como las redes neuronales biológicas del cerebro humano procesan la información.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpU2AKFzg3rZ"
      },
      "source": [
        "### Una sola neurona\n",
        "\n",
        "La unidad básica de cálculo en una red neuronal es la *neurona*, a menudo llamada nodo o unidad. Recibe información de algunos otros nodos, o de una fuente externa y calcula una salida. Cada entrada tiene un peso asociado (`w`), que se asigna en función de su importancia relativa a otras entradas. El nodo aplica una función `f` a la suma ponderada de sus entradas como se muestra a continuación:\n",
        "\n",
        "![](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-3-42-21-am.png?w=1136&h=606)\n",
        "\n",
        "La red anterior toma entradas numéricas `X1` y `X2` y tiene pesos `w1` y `w2` asociados con esas entradas. Además, hay otra entrada `1` con el peso `b` (llamado *bias*) asociado a ella, y por el momento vamos a omitir esta entrada.\n",
        "\n",
        "La salida `Y` de la neurona se calcula como se muestra en la figura. La función `f` se llama *función de activación*. Una función de activación toma un número y realiza una operación matemática determinada. Hay varias funciones de activación que se pueden encontrar en la práctica:\n",
        "\n",
        "* `Sigmoide`: toma una entrada de valor real y la aplasta para que oscile entre 0 y 1\n",
        "\n",
        "* `ReLU`: ReLU significa *unidad lineal rectificada*. Toma una entrada de valor real y la umbraliza a cero (reemplaza los valores negativos con cero)\n",
        "\n",
        "![](https://miro.medium.com/max/1742/1*XxxiA0jJvPrHEJHD4z893g.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "059yXzghgr6Z"
      },
      "source": [
        "### Red neuronal feedforward multicapa (MLP)\n",
        "\n",
        "La red neuronal feedforward fue el primer modelo ideado y a su vez el más simple. Contiene múltiples neuronas (nodos) dispuestas en capas. Los nodos de las capas adyacentes tienen conexiones entre ellos y todas estas conexiones tienen pesos asociados con ellas. Un ejemplo de una red neuronal feedforward se muestra en la figura:\n",
        "\n",
        "![](https://miro.medium.com/max/1200/1*BQ0SxdqC9Pl_3ZQtd3e45A.png)\n",
        "\n",
        "Una red neuronal feedforward (multilayer) consta de nodos en tres tipos de capas:\n",
        "\n",
        "1. *Capa de entrada*: los nodos de esta capa proporcionan información externa a la red. No se realiza ningún cálculo en estos nodo, simplemente transmiten la información a los nodos de la capa oculta.\n",
        "\n",
        "2. *Capas ocultas*: los nodos de estas capas realizan cálculos y transfieren información de los nodos de entrada a los nodos de salida. Una red feedforward puede tener cero o múltiples capas ocultas.\n",
        "\n",
        "3. *Capa de salida*: los nodos de salida son responsables de los cálculos y la transferencia de información de la red al mundo exterior.\n",
        "\n",
        "En una red feedforward, la información se mueve en una sola dirección, hacia adelante, desde los nodos de entrada, a través de los nodos ocultos y hacia los nodos de salida. No hay ciclos ni bucles en la red."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk9AyYeDiYLZ"
      },
      "source": [
        "### Entrenando una ANN: algoritmo Backpropagation\n",
        "\n",
        "El proceso por el cual una ANN aprende se conoce como algoritmo de *backpropagation*, o retropropagación (abreviado como *BackProp*):\n",
        "\n",
        "* Es un esquema de entrenamiento supervisado, lo que significa que aprende de datos de entrenamiento ya etiquetados.\n",
        "* En términos simples, BackProp consiste en \"aprender de los errores\". El algoritmo corrige el ANN cada vez que comete errores.\n",
        "* El objetivo del aprendizaje es asignar pesos correctos a las conexiones entre nodos de capas diferentes. Dado un vector de entrada, estos pesos determinan cuál es el vector de salida.\n",
        "\n",
        "#### Algoritmo BackProp\n",
        "\n",
        "![](https://miro.medium.com/max/600/0*zjTuE91Skw7y2Vx-)\n",
        "Fuente: https://gfycat.com/gifs/search/backpropagation\n",
        "\n",
        "\n",
        "Inicialmente, todos los pesos de borde se asignan aleatoriamente. Para cada entrada en el conjunto de datos de entrenamiento, el ANN se activa y se observa su salida. Esta salida se compara con la salida deseada que ya conocemos, y el error se *propaga* de nuevo a la capa anterior. Este error se observa y los pesos se *ajustan* en consecuencia. Este proceso se repite hasta que el error de salida esté por debajo de un umbral predeterminado.\n",
        "\n",
        "#### Predicción de nuevas entradas\n",
        "\n",
        "Una vez que termina el algoritmo BackProp, hemos aprendido una ANN, y está lista para trabajar con nuevas entradas. La siguiente figura muestra como fluye la información de una nueva entrada para una red que ya fue entrenada.\n",
        "\n",
        "![](https://miro.medium.com/max/1111/0*bmcR3nOLvyp1moa6.gif)\n",
        "\n",
        "Con el fin de mejorar el entendimiento de este proceso, se puede utilizar la web desarrollada por Adam Harley (http://scs.ryerson.ca/~aharley/vis/fc/) para visualizar en 3D una ANN que fue entrenada (usando Backpropagation) para reconocer dígitos escrito a mano sobre la base de datos MNIST.\n",
        "\n",
        "La red toma 784 valores numéricos de píxeles como entradas de una imagen de 28x28 de un dígito escrito a mano (tiene 784 nodos en la capa de entrada correspondientes a píxeles). La red tiene 300 nodos en la primera capa oculta, 100 nodos en la segunda capa oculta y 10 nodos en la capa de salida (correspondientes a los 10 dígitos)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtMBpJNcqWUE"
      },
      "source": [
        "## Problemas de las ANN tradicionales (MLP)\n",
        "\n",
        "Antes de empezar con la páctica de ANN para análisis de imágenes, vamos a repasar los problemas que presentan las ANN tradicionales.\n",
        "\n",
        "* Hay varios inconvenientes con las MLP, especialmente cuando se trata del procesamiento de imágenes. Los MLP usan un perceptrón para cada entrada (por ejemplo, píxel en una imagen, multiplicado por 3 en caso de RGB). La cantidad de pesos rápidamente se vuelve inmanejable para imágenes grandes. Para una imagen de 224x224 píxeles con 3 canales de color, hay alrededor de 150.000 pesos que deben entrenarse. Como resultado, surgen dificultades durante el entranamiento, como ser el *sobreajuste*.\n",
        "\n",
        "* Otro problema común es que los MLP reaccionan de manera diferente a una entrada (imágenes) y su versión desplazada: no son invariantes de traslación. Por ejemplo, si aparece una imagen de un gato en la parte superior izquierda de una imagen y otro en la parte inferior derecha de otra imagen, el MLP intentará corregirse y asumirá que siempre aparecerá un gato en esa sección de la imagen.\n",
        "\n",
        "Ahora si, ya podemos comenzar con la práctica de este laboratorio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbVhjPpzn6BM"
      },
      "source": [
        "# Clasificación con ANN: predecir la clase de una imagen\n",
        "\n",
        "Tutorial extraído de https://www.tensorflow.org/tutorials/keras/classification\n",
        "\n",
        "En esta primera parte del laboratorio vamos entrenar una red neuronal para clasificar imagenes de ropa como ser zapatos, vestidos, camisetas y más. No hay problema sino entienden todos los detalles, ya que es una mirada rápido sobre un programa completo de *Tensorflow* con los detalles explicados a medida que se avanza.\n",
        "\n",
        "Esta Guia usa [tf.keras](https://www.tensorflow.org/guide/keras), una API de alto nivel para construir y entrenar modelos de redes neuronales en Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL3OqFKZ9dFg"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# TensorFlow y tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Librerias de ayuda\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## Importar el set de datos de moda de MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLdCchMdCaWQ"
      },
      "source": [
        "Esta guia usa el set de datos de [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)\n",
        "que contiene más de 70000 imágenes en 10 categorias. Las imágenes muestran artículos individuales de ropa a una resolución muy baja (28 por 28 pixeles), tal como se ve aca:\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://miro.medium.com/max/1400/1*RkysnlFejHNE4Us5aXmnHQ.jpeg\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n",
        "\n",
        "En este lab vamos a usar 60.000 imágenes para entrenar la red neuronal y 10.000 para evaluar que tan exacto aprendió la red a clasificar imágenes de moda. Se puede acceder al dataset Fashion MNIST directamente desde TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MqDQO0KCaWS"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9FDsUlxCaWW"
      },
      "source": [
        "Al cargar el dataset, la función retorna cuatro arreglos en `NumPy`:\n",
        "\n",
        "* Los arreglos `train_images` y `train_labels` son los datos del *training set* que se usan para aprender el modelo.\n",
        "* Los arreglos `test_images` y `test_labels` son los datos del *test set* que se usan para probar el modelo.\n",
        "\n",
        "Las imágenes se cargan como arreglos `NumPy` de 28x28, con valores de pixel que varian de 0 a 255 (imagen en escala de grises). Los arreglos *labels* son arreglos de enteros, que van del 0 al 9, con tantas filas como imágenes hay para entrenamiento y para testeo, respectivamente. Estos corresponden a la *clase* de ropa que la imagen representa:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "Cada imagen es mapeada a una unica etiqueta. Ya que los nombres de las clases no estan incluídos en los arreglos *labels*, los almacenamos acá para usarlos luego cuando se visualicen las imágenes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjnLH5S2CaWx"
      },
      "outputs": [],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brm0b_KACaWX"
      },
      "source": [
        "## Explorar el set de datos\n",
        "\n",
        "Exploramos el la información del dataset antes de entrenar el modelo. Lo siguiente muestra que hay 60.000 imágenes en el set de entrenamiento con cada imagen representada por un arrelgo de 28x28 píxeles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW5k_xz1CaWX"
      },
      "outputs": [],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNJt3CAh2Jz_"
      },
      "outputs": [],
      "source": [
        "train_images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIAcvQqMCaWf"
      },
      "source": [
        "Asimismo, hay 60.000 etiquetas en el set de entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRFYHB2mCaWb"
      },
      "outputs": [],
      "source": [
        "len(train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSlYxFuRCaWk"
      },
      "source": [
        "Cada etiqueta es un entero entre 0 y 9:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKnCTHz4CaWg"
      },
      "outputs": [],
      "source": [
        "train_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMPI88iZpO2T"
      },
      "source": [
        "Hay 10.000 imágenes en el set de pruebas. Otra vez, cada imagen es representada por un arrelgo de 28x28 píxeles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KFnYlcwCaWl"
      },
      "outputs": [],
      "source": [
        "test_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd0A0Iu0CaWq"
      },
      "source": [
        "Y el set de pruebas contiene 10.000 etiquetas de imagen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJmPr5-ACaWn"
      },
      "outputs": [],
      "source": [
        "len(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels"
      ],
      "metadata": {
        "id": "IfXvFH4FR4WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES6uQoLKCaWr"
      },
      "source": [
        "## Pre-procesar el dataset\n",
        "\n",
        "El set de datos debe ser pre-procesado antes de entrenar la red. Al visualizar la primera imagen en el set de entrenamiento, encontramos que los valores de los pixeles están entre 0 y 255:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4VEw8Ud9Quh"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz7l27Lz9S1P"
      },
      "source": [
        "Antes de entrenar la red neuronal debemos escalar estos valores en un rango de 0 a 1. Para hacero, dividimos los valores por 255. Es importante que el *training set* y el *testing set* se pre-procesen de la misma forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW5WzIPlCaWv"
      },
      "outputs": [],
      "source": [
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee638AlnCaWz"
      },
      "source": [
        "Para verificar que el set de datos está en el formato adecuado y que están listos para entrenar la red, veamos las primeras 25 imágenes de el *training set* junto al nombre de clase debajo de cada imagen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZTImqg_CaW1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.gray) #\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59veuiEZCaW4"
      },
      "source": [
        "## Construir del Modelo\n",
        "\n",
        "Construir la red neuronal requiere configurar las capas del modelo y luego compilarlo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxg1XGm0eOBy"
      },
      "source": [
        "### Configurar las Capas\n",
        "\n",
        "Los bloques de construcción básicos de una red neuronal son las *capas* o *layers*. Las capas extraen representaciones (características o features) del set de datos con el que se alimentan.\n",
        "\n",
        "La mayor parte del *aprendizaje profundo* (deep learning) consiste en encadenar capas simples. La mayoría de las capas tienen parámetros que son aprendidos durante el entrenamiento. En TensorFlow, estas capas son implementadas mediante `tf.keras.layers.Dense`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ODch-OFCaW4"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gut8A_7rCaW6"
      },
      "source": [
        "La primera capa de esta red, `tf.keras.layers.Flatten`,\n",
        "transforma las imágenes de un arreglo bi-dimensional (de 28x28 píxeles) a un arreglo uni-dimensional (de 28\\*28 píxeles = 784 píxeles). En esta capa no hay parámetros que aprender, solo se *reformatea* el set de datos.\n",
        "\n",
        "Despues de que los píxeles son \"aplanados\", el modelo consiste de una secuencia de dos capas `tf.keras.layers.Dense`. Estas están densamente conectadas, o completamente conectadas. La primera capa `Dense` tiene 128 nodos (o neuronas) con una *función de activación* del tipo *relu*. La segunda (y última) capa es una capa de 10 nodos on una *función de activación* de tipo *softmax*, que devuelve un arreglo de 10 probabilidades que suman 1. Cada nodo contiene una *calificación* que indica la probabilidad de que la imagen actual pertenezca a una de las 10 clases.\n",
        "\n",
        "### Compilar el modelo\n",
        "\n",
        "Antes de que el modelo este listo para ser entrenado, se necesitan algunas configuraciones más. Estas son agregadas durante el paso de compilacion del modelo:\n",
        "\n",
        "* *Loss function*: esto permite medir que tan exacto es el modelo durante el entrenamiento. La idea es minimizar el valor de esta función para \"dirigir\" el modelo en la direccion adecuada.\n",
        "* *Optimizer*: así es como se actualiza el modelo basado en los datos de entrenamiento y su loss function.\n",
        "* *Metrics*: se usan para monitorear los pasos de entrenamiento y de testeo. El siguiente ejemplo usa *accuracy* (exactitud), es decir, la fracción de imágenes que son correctamente clasificadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lhan11blCaW7"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKF6uW-BCaW-"
      },
      "source": [
        "## Entrenar el Modelo\n",
        "\n",
        "Entrenar la red neuronal requiere de los siguientes pasos:\n",
        "\n",
        "1. Entregar los datos de entrenamiento al modelo. En este ejemplo, el set de datos de entrenamiento estan en los arreglos `train_images` y `train_labels`.\n",
        "2. Correr el entreanamiento para que el modelo aprenda a asociar imágenes con etiquetas.\n",
        "3. Pedir al modelo que haga predicciones sobre un set de datos de pruebas, incluido en el arreglo `test_images`. Verificar que las predicciones sean iguales a las etiquetas del arreglo `test_labels`.\n",
        "\n",
        "Para comenzar a entrenar, se llama el método `model.fit`. Se denomina así por que ajusta (*fit*) el modelo a el set de datos de entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvwvpA64CaW_"
      },
      "outputs": [],
      "source": [
        "model.fit(train_images, train_labels, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3ZVOhugCaXA"
      },
      "source": [
        "A medida que el modelo entrena, la muestran los valores de *loss function* y *accuracy* sobre el **set de datos de entrenamiento**.\n",
        "\n",
        "En este punto vale aclarar terminología de redes neuronales, y que son parámetros del método `fit`:\n",
        "\n",
        "* Un `epoch`: un pase hacia adelante (*forward pass*) y un pase hacia atrás (*backward pass*) de **todos los ejemplos de entrenamiento**.\n",
        "* `batch size`: número de ejemplos de entrenamiento en un forward/backward pass. Cuanto mayor sea el tamaño del lote, más espacio en memoria se necesita.\n",
        "* Número de `iteraciones` = número de pasadas, donde el número de ejemplos de involucrados en cada pasada esta especificado por `[batch size]`. Una pasada es un *forward pass* mas un *backward pass*.\n",
        "\n",
        "Ejemplo: si se tiene 1.000 ejemplos de entrenamiento y el `batch size` es de 500, entonces se necesitan 2 iteraciones para completar 1 `epoch`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEw4bZgGCaXB"
      },
      "source": [
        "## Evaluar Accuracy\n",
        "\n",
        "A continuación vamos a estimar el rendimiento del modelo sobre el set de datos de prueba:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VflXLEeECaXC"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWfgsmVXCaXG"
      },
      "source": [
        "Resulta que la *accuracy* sobre el set de datos de pruebas es un poco menor que la *accuracy* sobre el set de entrenamiento. Esta diferencia entre el entrenamiento y el test se debe al *overfitting* (sobre ajuste). Sobre ajuste sucede cuando un modelo de aprendizaje de maquina (ML) tiene un rendimiento peor sobre un set de datos nuevo (o de testeo), que nunca antes ha visto comparado con el de entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsoS7CPDCaXH"
      },
      "source": [
        "## Hacer predicciones\n",
        "\n",
        "Finalmente, con el modelo entrenado ya se pueden hacer predicciones sobre nuevas imágenes. Para esto usamos el método `predict` con las imágenes del conjunto de testeo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl91RPhdCaXI"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(test_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9Kk1voUCaXJ"
      },
      "source": [
        "En la celda anterior, el modelo predice la etiqueta para cada imagen en el set de datos de prueba. Miremos la primera prediccion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DmJEUinCaXK"
      },
      "outputs": [],
      "source": [
        "predictions[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hw1hgeSCaXN"
      },
      "source": [
        "Una prediccion es un arreglo de 10 números (que corresponden a las 10 neuronas de la segunda capa). Estos valores representan el nivel de \"confianza\" (o probabilidad) que el modelo arroja sobre las imágenes de cada uno de los 10 artículos de moda/ropa. Se puede revisar cual tiene el nivel más alto de confianza:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsqenuPnCaXO"
      },
      "outputs": [],
      "source": [
        "pred_label = np.argmax(predictions[0])\n",
        "print(pred_label, \" [\", class_names[pred_label], \"]\", sep=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E51yS7iCCaXO"
      },
      "source": [
        "De esta manera, el modelo tiene mayor confianza que esta imagen es una \"ankle boot\". Examinando las etiquetas del set de datos de pruebas, podemos ver que esta clasificaion es correcta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd7Pgsu6CaXP"
      },
      "outputs": [],
      "source": [
        "test_labels[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygh2yYC972ne"
      },
      "source": [
        "Por último vamos crear una función que nos permita graficar la probabilidad de clase de cada una de las imagenes del dataset de pruebas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvYmmrpIy6Y1"
      },
      "outputs": [],
      "source": [
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  predictions_array, true_label = predictions_array, true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Ov9OFDMmOD"
      },
      "source": [
        "Tomemos como ejemplos la primer imagen del conjunto de testeo (`i=0`) y la décimotercera (`i=12`). Las etiquetas de prediccion correctas estan en azul y las incorrectas estan en rojo. El numero entrega el porcentaje para la etiqueta predicha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV5jw-5HwSmO"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions[i], test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions[i],  test_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko-uzOufSCSe"
      },
      "outputs": [],
      "source": [
        "i = 12\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions[i], test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions[i],  test_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgdvGD52CaXR"
      },
      "source": [
        "Ahora vamos a graficar multiples imágenes con sus predicciones. Notese que el modelo puede estar equivocado aun cuando tiene mucha confianza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQlnbqaw2Qu_"
      },
      "outputs": [],
      "source": [
        "# Plot the first X test images, their predicted labels, and the true labels.\n",
        "# Color correct predictions in blue and incorrect predictions in red.\n",
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(i, predictions[i], test_labels, test_images)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(i, predictions[i], test_labels)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R32zteKHCaXT"
      },
      "source": [
        "Finalmente, usamos el modelo entrenado para hacer una predicción sobre una única imagen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRJ7JU7JCaXT"
      },
      "outputs": [],
      "source": [
        "# Grab an image from the test dataset.\n",
        "img = test_images[1]\n",
        "\n",
        "print(img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz3bVp21CaXV"
      },
      "source": [
        "Los modelos de `tf.keras` se optimizan para hacer predicciones en un lote de ejemplos a la vez. En consecuencia, a pesar de que está utilizando una sola imagen, es necesario añadirla a una lista:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDFh5yF_CaXW"
      },
      "outputs": [],
      "source": [
        "# Add the image to a batch where it's the only member.\n",
        "img = (np.expand_dims(img,0))\n",
        "\n",
        "print(img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ5wLTkcCaXY"
      },
      "source": [
        "Ahora se puede predecir la etiqueta correcta para esta única imagen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_rzNSdrCaXY"
      },
      "outputs": [],
      "source": [
        "predictions_single = model.predict(img)\n",
        "\n",
        "print(predictions_single)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ai-cpLjO-3A"
      },
      "outputs": [],
      "source": [
        "plot_value_array(1, predictions_single[0], test_labels)\n",
        "_ = plt.xticks(range(10), class_names, rotation=45)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU1Y2OAMCaXb"
      },
      "source": [
        "`model.predict` retorna una lista de listas para cada imagen dentro del lote de imáágenes. Por lo tanto, para tomar la prediccion de la única imagen dentro del lote hacemos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tRmdq_8CaXb"
      },
      "outputs": [],
      "source": [
        "pred_label_single = np.argmax(predictions_single[0])\n",
        "print(pred_label_single, \" [\", class_names[pred_label_single], \"]\", sep=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFc2HbEVCaXd"
      },
      "source": [
        "Y el modelo predice la etiqueta 2 (pullover)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgiWQh4WPbDL"
      },
      "source": [
        "---\n",
        "\n",
        "# Trabajo Práctico 1 (primera parte)\n",
        "---\n",
        "Facundo Melendi Suarez  DNI:40835638,   facundomelendi@unc.edu.ar\n",
        "\n",
        "Esequiel Armneria  DNI: 35190750, esequiel1308@gmail.com\n",
        "\n",
        "\n",
        "Marianela Carubelli DNI: 26103422 mcarubelli@unc.edu.ar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j3DS0bPOXJd"
      },
      "source": [
        "## **EJERCICIO 1.1**: predecir nuevas imágenes de moda con el modelo previamente entrenado\n",
        "\n",
        "1. **Crear un nuevo dataset** propio con **30 imágenes** en total (3 imágenes de cada categoría). Estás imágenes no deben ser tomadas del dataset original (Fashion MNIST). Pueden capturar sus propias imágenes o buscarlas en la web. **Como sea, recuerden que deben preprocesar las imágenes para que tengan exactamente el mismo formato de entrada que requiere la red.**\n",
        "\n",
        "1. **Mostrar todas las imágenes del conjunto de testeo** creado por ustedes para que se pueda inspeccionar rápidamente su contenido.\n",
        "\n",
        "1. Tomando la ANN previamente entrenada, **predecir las etiquetas** de cada imagen del dataset y reportar los resultados de **accuracy** (y **opcionalmente** cualquier otra métrica que le resulte adecuada, como ser **precisión** y **recall**)\n",
        "\n",
        "TIP: reutilice las celdas de código presentadas anteriormente\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las imagenes utilizadas se encuentran en el siguiente repositorio\n",
        "\n",
        "\n",
        "https://github.com/Odnucaf/Diplo_Datos/tree/07a043d7781df92ebf9bc1e31e97b9e8728f560e/imagenes%20practico"
      ],
      "metadata": {
        "id": "nBT3WGrUY6be"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos las imágenes que bajamos de google:"
      ],
      "metadata": {
        "id": "wK2HrkjvSt1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Clona el repositorio\n",
        "!git clone https://github.com/Odnucaf/Diplo_Datos.git\n",
        "\n",
        "# 2. Navega al directorio que contiene las imágenes\n",
        "%cd Diplo_Datos/imagenes\\ practico\n",
        "\n",
        "# 3. Copia las imágenes al directorio /content/imagenes/\n",
        "!cp -r . /content/imagenes/\n",
        "\n",
        "# Verifica si las imágenes están en el directorio /content/imagenes/\n",
        "!ls /content/imagenes/"
      ],
      "metadata": {
        "id": "7nRnryrP7Ye3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2Nsx7wS2J0I"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Ruta de la carpeta\n",
        "carpeta = '/content/imagenes/'\n",
        "\n",
        "# Obtener los nombres de todos los archivos y carpetas en la carpeta\n",
        "nombres = os.listdir(carpeta)\n",
        "\n",
        "# Obtener las direcciones completas de cada archivo o subcarpeta\n",
        "direcciones = [os.path.join(carpeta, nombre) for nombre in nombres]\n",
        "\n",
        "print(direcciones)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertimos las imágenes a escala de grises, invertimos la escala para que el fondo sea negro, y normalizamos los pixeles para que tengan valores de 0 a 1."
      ],
      "metadata": {
        "id": "Q2UVqi61S2Tp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bn0Zg1c2J0I"
      },
      "outputs": [],
      "source": [
        "resultado = np.empty((0, 28, 28))\n",
        "for imag in direcciones:\n",
        "    img = cv2.imread(imag, cv2.IMREAD_GRAYSCALE)\n",
        "    img =  cv2.resize(img, (28,28))\n",
        "    img = 255 - img\n",
        "    img = img/255\n",
        "    plt.imshow(img, cmap='gray') #,\n",
        "    img = (np.expand_dims(img,0))\n",
        "    resultado = np.append(resultado, img, axis=0)\n",
        "resultado.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos como quedaron las 30 imagenes procesadas:"
      ],
      "metadata": {
        "id": "rs_2mVLTTIXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(30):\n",
        "    plt.subplot(6,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(resultado[i], cmap=plt.cm.gray) #\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TabYBcFr7OhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos el modelo entrenado previamente para predecir las etiquetas de nuestras 30 imágenes."
      ],
      "metadata": {
        "id": "cj9bpk2qYCjX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g6efclk2J0I"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCVqapQC2J0I"
      },
      "outputs": [],
      "source": [
        "df_resultados = pd.DataFrame()\n",
        "\n",
        "df_resultados['Prediccion'] = ''\n",
        "df_resultados['Valor_real'] = ''\n",
        "\n",
        "#df_resultados['Prediccion'] = df_resultados['Prediccion'].astype(str)\n",
        "#df_resultados['Valor_real'] = df_resultados['Valor_real'].astype(str)\n",
        "\n",
        "for d, d2 in enumerate(direcciones):\n",
        "    df_resultados.loc[d,['Prediccion']] = str(class_names[np.argmax(predictions[d])])\n",
        "    df_resultados.loc[d,['Valor_real']] = str(direcciones[d])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardamos en un datframe llamado df_resultados las etiquetas reales y las predichas por el modelo."
      ],
      "metadata": {
        "id": "T6wwTxLrTScW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYnuW3Op2J0I"
      },
      "outputs": [],
      "source": [
        "# Elimina '.jpg'\n",
        "df_resultados['Valor_real'] = df_resultados['Valor_real'].str.replace('.jpg', '')\n",
        "\n",
        "# Elimina 'imagenes practico/'\n",
        "df_resultados['Valor_real'] = df_resultados['Valor_real'].str.replace('/content/imagenes/', '')\n",
        "\n",
        "# Elimina todos los dígitos\n",
        "df_resultados['Valor_real'] = df_resultados['Valor_real'].str.replace(r'\\d+', '', regex=True)\n",
        "\n",
        "# Elimina paréntesis\n",
        "df_resultados['Valor_real'] = df_resultados['Valor_real'].str.replace(r'\\(|\\)', '', regex=True)\n",
        "\n",
        "#Vamos a agregar la barra en T-shirttop para que pueda leerlo igual que a las categorias\n",
        "df_resultados['Prediccion'] = df_resultados['Prediccion'].replace('T-shirttop', 'T-shirt/top')\n",
        "df_resultados['Valor_real'] = df_resultados['Valor_real'].replace('T-shirttop', 'T-shirt/top')\n",
        "\n",
        "df_resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agregamos a df_resultados los labels numéricos de esas etiquetas para podes calcular métricas por clases."
      ],
      "metadata": {
        "id": "cs5bO3c_TbmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorias = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "mapeo = {categoria: idx for idx, categoria in enumerate(categorias)}\n",
        "\n",
        "df_resultados['label_real'] = df_resultados['Valor_real'].map(mapeo)\n",
        "df_resultados['label_pred'] = df_resultados['Prediccion'].map(mapeo)\n",
        "\n",
        "df_resultados= df_resultados.fillna(2) #por alguna razón que no descubro a 'Pullover' no lo reconoce y lo pone como NaN, por eso lo reemplazo por 2 que es el label que le corresponde :(\n",
        "df_resultados['label_real'] = df_resultados['label_real'].astype(int)\n",
        "df_resultados"
      ],
      "metadata": {
        "id": "XiFvn2EVJeum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las imágenes reales y su etiqueta, junto con la imagen procesada y la etiqueta predicha, para las 30 imágenes."
      ],
      "metadata": {
        "id": "87CwQPssTmoh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg8ggZjv2J0J"
      },
      "outputs": [],
      "source": [
        "i = 22\n",
        "\n",
        "# Leer las imágenes\n",
        "imagen_original = np.squeeze(cv2.imread(direcciones[i]))\n",
        "imagen_resultado = np.squeeze(resultado[i])\n",
        "\n",
        "# Crear un arreglo de subgráficos (1 fila, 2 columnas)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
        "\n",
        "# Mostrar la imagen original en el primer subgráfico\n",
        "ax1.imshow(imagen_original, cmap='gray')\n",
        "ax1.set_title(('Valor_real ---->   '+df_resultados['Valor_real'][i]))\n",
        "\n",
        "# Mostrar la imagen resultado en el segundo subgráfico\n",
        "ax2.imshow(imagen_resultado, cmap='gray')\n",
        "ax2.set_title(('Prediccion ---->   '+df_resultados['Prediccion'][i]))\n",
        "\n",
        "# Ajustar el espacio entre subgráficos\n",
        "plt.tight_layout()\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1xcvPfJ2J0J"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "for i in range(30):  # i irá desde 0 hasta 29\n",
        "    # Leer las imágenes\n",
        "    imagen_original = np.squeeze(cv2.imread(direcciones[i]))\n",
        "    imagen_resultado = np.squeeze(resultado[i])\n",
        "\n",
        "    # Crear un arreglo de subgráficos (1 fila, 2 columnas)\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(4, 4))\n",
        "\n",
        "    # Mostrar la imagen original en el primer subgráfico\n",
        "    ax1.imshow(imagen_original, cmap='gray')\n",
        "    ax1.set_title(('Valor_real ---->   '+df_resultados['Valor_real'][i]))\n",
        "\n",
        "    # Mostrar la imagen resultado en el segundo subgráfico\n",
        "    ax2.imshow(imagen_resultado, cmap='gray')\n",
        "    ax2.set_title(('Prediccion ---->   '+df_resultados['Prediccion'][i]))\n",
        "\n",
        "    # Ajustar el espacio entre subgráficos\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Mostrar el gráfico\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculemos el accuracy general en la predición:"
      ],
      "metadata": {
        "id": "1FtR47FGTyhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "label_pred = df_resultados['label_pred']\n",
        "label_real = df_resultados['label_real']\n",
        "testB_loss, testB_acc = model.evaluate(resultado,  label_real, verbose=2)\n",
        "\n",
        "\n",
        "print('\\nNuevo accuracy:', testB_acc)"
      ],
      "metadata": {
        "id": "PSonGotZaRrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora vamos a calcular la precisión y el recall por cada clase:\n"
      ],
      "metadata": {
        "id": "XnFMTIckT409"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapeo = {idx: categoria for idx, categoria in enumerate(categorias)}\n",
        "df_metricas = pd.DataFrame()\n",
        "\n",
        "# Calcular el accuracy por clase\n",
        "accuracy_por_clase = []\n",
        "for clase in range(10):  # Suponiendo 10 clases (0 al 9)\n",
        "    accuracy_clase = accuracy_score(label_real[label_pred == clase], label_pred[label_pred == clase])\n",
        "    accuracy_por_clase.append(accuracy_clase)\n",
        "    #df_metricas[clase]=clase\n",
        "    df_metricas.at[clase, 'precisión_clase'] = accuracy_clase\n",
        "    # print(f'Clase {clase}: Accuracy={accuracy_clase}')\n",
        "\n",
        "# Calcula el recall por clase\n",
        "\n",
        "recall_por_clase = []\n",
        "for clase in range(10):\n",
        "    recall_clase = recall_score(label_real[label_real == clase], label_pred[label_real == clase], average='micro') #\n",
        "    recall_por_clase.append(recall_clase)\n",
        "    df_metricas.at[clase, 'recall_clase'] = recall_clase\n",
        "\n",
        "\n",
        "# Imprime los resultados\n",
        "for clase, accuracy, recall in zip(range(10), accuracy_por_clase, recall_por_clase):\n",
        "    print(f'Clase {clase}: Precisión={accuracy}, Recall={recall}')\n",
        "df_metricas['categoria'] = df_metricas.index.map(mapeo)\n",
        "df_metricas = df_metricas[['categoria'] + [col for col in df_metricas if col != 'categoria']]"
      ],
      "metadata": {
        "id": "XUHOa8SQiVkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_resultados"
      ],
      "metadata": {
        "id": "k59zeuZt7_YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos la precision y el recall por cada clase, almacenado en el datframe df_metricas:"
      ],
      "metadata": {
        "id": "CyKcyyPdUF_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_metricas"
      ],
      "metadata": {
        "id": "R0XZhJzB_Wau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaUCC_ntBMtj"
      },
      "source": [
        "## **EJERCICIO 1.2**: informe y conclusiones del ejercicio 1.1\n",
        "\n",
        "1. Elabore un breve resumen de lo realizado en el ejercicio 1.1\n",
        "\n",
        "2. Describa los principales resultados obtenidos\n",
        "\n",
        "3. Presente (si es posible) otros hallazgos realizados\n",
        "\n",
        "4. Escriba las conclusiones finales"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Informe del ejercicio 1:\n",
        "\n",
        "* Se bajaron de google 30 imágenes de ropa, que tuviesen las etiquetas de las 10 clases planteadas en el problema.\n",
        "\n",
        "* Se subieron las imágenes a una carpeta local del colab, las leimos y transformamos sus 3 canales a un solo canal en escala de grises.\n",
        "\n",
        "* Luego se redimensionaron las imágenes al tamaño de 28x28, se invirtió la escala de color para que el fondo sea negro, y luego se normalizó la escala de los pixeles para que tengan valores de 0 a 1.\n",
        "\n",
        "* A continuación creamos una matriz numpy de (30, 28, 28) que contiene todos los pixeles de las 30 imágenes de 28x28, con el nombre **resultado**.\n",
        "\n",
        "* Usamos el modelo entrenado previamente con el set de datos de moda de MNIST, para predecir las etiquetas de las 30 imágenes nuevas.\n",
        "\n",
        "* Guardamos en el datframe df_resultados las etiquetas predichas, las etiquetas reales, y los labels numéricos de esas etiquetas predichas y reales.\n",
        "\n",
        "* Mostramos las 30 imágenes reales junto a las trasnformadas, con la etiqueta real y la etiqueta predicha por el modelo, para que se pueda inspeccionar rápidamente el contenido y el resultado.\n",
        "\n",
        "* Medimos el accuracy de este lote de 30 imagenes, que dio muchísimo menos que el de train o test de antes. Ahora el accuracy da solamente 0.23.\n",
        "\n",
        "* Además calculamos el precisión y recall por clase (es decir analizando sólo por cada clase) y guardamos estos resultados en df_metricas. Estas métricas son complicadas de analizar ya que sólo tenemos 3 imágenes de cada clase, es decir muy poca estadística.\n",
        "\n",
        "\n",
        "**Conclusiones**: Pudimos etiquetar y evaluar el proceso de etiquetado de un lote nuevo de imágenes. El accuracy dio bajísimo, comparado con el accuracy obtenido en test. Esto seguramengte se debe a que nuestro lote de imágenes no tiene ningún proceso de curación.\n",
        "Pareciera que el modelo tiende a predecir con más frecuencia a la categoria Bag, aunque no pudimos relacionar eso con las métricas precisión ni recall por clase. La categoría que tiene mejor precisión es Dress. Sin embargo estos análisis sólo se han hecho sobre 30 imágenes, es decir con muy poca estadística."
      ],
      "metadata": {
        "id": "EwsgiwjvZ9S_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMv1VZqcUWGQ"
      },
      "source": [
        "## **EJERCICIO 1.3 (OPCIONAL)**: entrenar la red para resolver un nuevo problema\n",
        "\n",
        "Realice una búsqueda en internet para relevar dataset similares al propuesto en este lab. Elija un dataset arbitario y con este vuelva a realizar el proceso de completo para entrenar una ANN como clasificador de imágenes sobre el nuevo dateset\n",
        "\n",
        "TIP: dejo dos links para facilitar la búsqueda\n",
        "- https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research\n",
        "- https://en.wikipedia.org/wiki/Caltech_101\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se utilizo el dataset sobre manos. La idea de este ejercicio es determinar dependendiendo de la imagen, a que sexo pertenece la mano.\n",
        "https://sites.google.com/view/11khands"
      ],
      "metadata": {
        "id": "CJ_LmOybVu6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa un nuevo repositorio en tu entorno de Colab\n",
        "!git init\n",
        "\n",
        "# Agrega el repositorio remoto\n",
        "!git remote add -f origin https://github.com/Mariandela/Diplomatura.git\n",
        "\n",
        "# Configura el repositorio para permitir \"sparse-checkout\"\n",
        "!git config core.sparseCheckout true\n",
        "\n",
        "# Especifica la carpeta que deseas descargar\n",
        "!echo \"hands_test/\" >> .git/info/sparse-checkout\n",
        "!echo \"hands_train/\" >> .git/info/sparse-checkout\n",
        "# Sincroniza con el repositorio remoto y descarga solo la carpeta especificada\n",
        "!git pull origin main"
      ],
      "metadata": {
        "id": "jhgSIiLOIXQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1f36lnOV-RB"
      },
      "outputs": [],
      "source": [
        "# Ruta de las carpetas\n",
        "carpeta_hands_train = 'hands_train/'\n",
        "carpeta_hands_test = 'hands_test/'\n",
        "\n",
        "# Redimensionamos las imagenes y las cambiamos a blanco y negro.\n",
        "\n",
        "def img_load(folder_name):\n",
        "    # Obtener los nombres de todos los archivos y carpetas en la carpeta\n",
        "    nombres = os.listdir(folder_name)\n",
        "\n",
        "    # Obtener las direcciones completas de cada archivo o subcarpeta\n",
        "    direcciones = [os.path.join(folder_name, nombre) for nombre in nombres]\n",
        "    resultado = np.empty((0, 28, 28))\n",
        "    for imag in direcciones:\n",
        "        img = cv2.imread(imag, cv2.IMREAD_GRAYSCALE)\n",
        "        img =  cv2.resize(img, (28,28))\n",
        "        img = 255 - img\n",
        "        img = img/255\n",
        "        ##  plt.imshow(img, cmap='gray')\n",
        "        img = (np.expand_dims(img,0))\n",
        "        resultado = np.append(resultado, img, axis=0)\n",
        "    return(resultado)\n",
        "\n",
        "\n",
        "# Obtenemos las imagenes para train y test:\n",
        "hands_result_train = img_load(carpeta_hands_train)\n",
        "hands_result_test = img_load(carpeta_hands_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MsRWTcs9x6EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observamos las dimensiones de nuestros arreglos, para comprobar que esten correctos."
      ],
      "metadata": {
        "id": "A1s3OynRVqT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hands_result_train.shape"
      ],
      "metadata": {
        "id": "dcuM1DGnVkZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenemso entonces un arreglo de dimensión (7376, 28, 28), ya que son 7376 imágenes de 28x28 pixeles"
      ],
      "metadata": {
        "id": "zS40RbZ3JYAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hands_result_test.shape"
      ],
      "metadata": {
        "id": "SD4i8LAKVl4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como el grupo de test tiene 3700 imágenes de 28x28 pixeles, este arreglo tiene (3700, 28, 28)"
      ],
      "metadata": {
        "id": "Ivb6YExEJiXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos un LabelEncoding para codificar los labels. En este caso son femenino o masculino.\n",
        "url = 'https://raw.githubusercontent.com/Mariandela/Diplomatura/main/HandInfo.csv'\n",
        "label = pd.read_csv(url)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "img_label = label[['gender']]\n",
        "img_label[\"gender\"] =le.fit_transform(img_label[\"gender\"])\n",
        "img_label_train = img_label['gender'][:7376]\n",
        "img_label_test = img_label['gender'][7376:]"
      ],
      "metadata": {
        "id": "MMjaWre1VniI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_label_train.shape"
      ],
      "metadata": {
        "id": "XKpzX0_mVxVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(7376,)"
      ],
      "metadata": {
        "id": "pb-8AKIUKTFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_label_test.shape"
      ],
      "metadata": {
        "id": "Iy4il4OkV0O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3700,)"
      ],
      "metadata": {
        "id": "sni_JXl-KT4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos el modelo, vamos a hacer 10 épocas:"
      ],
      "metadata": {
        "id": "nWVhxvq8V2cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(hands_result_train, img_label_train, epochs=10)"
      ],
      "metadata": {
        "id": "iK-1n9HAV4ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluamos el modelo:"
      ],
      "metadata": {
        "id": "T5AtVZCHV6Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_hands, test_acc_hands = model.evaluate(hands_result_test,  img_label_test, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "L-nyJeteV7ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Podemos ver que obtuvimos un accuracy de test de 0.88, lo cual nos parece un buen resultado."
      ],
      "metadata": {
        "id": "lTk00mCcJ3jV"
      }
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}