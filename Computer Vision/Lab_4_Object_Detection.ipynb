{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98rds-2OU-Rd"
      },
      "source": [
        "# ¿Que es Object Detection?\n",
        "\n",
        "Object detection (también llamado a veces *object recognition*) es un término general que se usa para describir una colección de tareas de computer vision que implican identificar **objetos semánticos de una cierta clase** (como humanos, edificios, perros o coches) en imágenes o videos digitales.\n",
        "\n",
        "Mientras que la *clasificación imágenes* implica predecir la clase de un objeto en una imagen (lo que ya hicimos en los labs 1 y 2), la *localización de objetos* implica identificar la ubicación de uno o más objetos en una imagen definiendo un *bounding box* (recuadro delimitador) alrededor de su extensión. La **detección de objetos** combina estas dos tareas para localizar y clasificar uno o más objetos en una imagen, tal como muestra la siguiente imagen.\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/3/38/Detected-with-YOLO--Schreibtisch-mit-Objekten.jpg)\n",
        "\n",
        "Ahora vamos a distinguir estas tres tareas de computer vision.\n",
        "\n",
        "**Image classification**: predice el tipo o clase de un objeto en una imagen.\n",
        "- *Entrada*: una imagen con un solo objeto.\n",
        "- *Salida*: una etiqueta de clase.\n",
        "\n",
        "**Object localization**: localiza la presencia de objetos en una imagen e indique su ubicación con un bounding box.\n",
        "- *Entrada*: una imagen con uno o más objetos.\n",
        "- *Salida*: una o más casillas delimitadoras (por ejemplo, definidas por un punto, ancho y altura).\n",
        "\n",
        "**Object detection**: localiza la presencia de objetos con un bounding box y su respectiva clase.\n",
        "- *Entrada*: una imagen con uno o más objetos.\n",
        "- *Salida*: una o más bounding box (definidas por un punto, ancho y altura), y una etiqueta de clase para cada uno.\n",
        "\n",
        "En la siguiente imagen se muestran diferentes tareas de computer vision y los resultados que producen.\n",
        "\n",
        "![](https://nanonets.com/blog/content/images/size/w1000/2020/08/59b6d0529299e.png)\n",
        "\n",
        "Ahora que ya manejamos una terminología básica vamos a pasar a ver como podemos implementar la tarea de detección de objetos con deep learning en la práctica, usando Python y TensorFlow.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As-4UhRJjA2s"
      },
      "source": [
        "---\n",
        "\n",
        "**NOTA**: NOTA: Este notebook ha sido creado a partir de recursos disponibles en la web que se listan a continuación. La fuente de las imágenes originales se puede consultar directamente el markdown insertado en la celda de texto.\n",
        "\n",
        "- https://machinelearningmastery.com/object-recognition-with-deep-learning/\n",
        "- https://en.wikipedia.org/wiki/Object_detection\n",
        "- https://www.tensorflow.org/hub/tutorials/tf2_object_detection?hl=en\n",
        "- https://www.tensorflow.org/lite/models/object_detection/overview?hl=en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOvvWAVTkMR7"
      },
      "source": [
        "# Object Detection con TensorFlow Hub\n",
        "\n",
        "[TensorFlow Hub (TF Hub)](https://tfhub.dev/) es un repositorio de modelos de aprendizaje automático previamente entrenados, listos para ser optimizados e implementarlos donde quieras con solo unas pocas líneas de código. Para saber más de TF Hub pueden ir a las [guías oficiales](https://www.tensorflow.org/hub/), o chequear el [siguiente tutorial](https://medium.com/ymedialabs-innovation/how-to-use-tensorflow-hub-with-code-examples-9100edec29af).\n",
        "\n",
        "En resumen, TF Hub pone a dispocisión una gran colección de modelos *out-of-the-box* para detección de objetos, cuyos detalles se pueden encontrar en [este link](https://tfhub.dev/s?module-type=image-object-detection). En este notebook vamos a hacer uso de estos modelos, como también funciones provistas por la [API de TF Hub](https://www.tensorflow.org/hub/api_docs/python/hub).\n",
        "\n",
        "En este tutorial vamos revisar los pasos para ejecutar diferentes modelos de detección disponibles en TF Hub sobre imágenes del dataset [COCO 2017](https://cocodataset.org/#explore).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPs64QA1Zdov"
      },
      "source": [
        "## Imports y configuraciones iniciales\n",
        "\n",
        "Como siempre, comenzamos con los imports iniciales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn5_uV1HLvaz"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras version\", keras.__version__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogyryF2lFBL"
      },
      "source": [
        "## Utilidades\n",
        "\n",
        "La siguiente celda de código crea algunas utilidades que se necesitarán más adelante:\n",
        "\n",
        "- Cargar una imagen.\n",
        "- Administrar los modelos en TF Hub.\n",
        "- Una lista de imágenes para propósitos de testeo (se pueden agregar todas las que se quiera).\n",
        "- Información extra del dataset [COCO 2017](https://cocodataset.org/#home) (necesario para modelos algunos modelos disponibles en TF Hub)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y9R0Xllefec"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  image = None\n",
        "  if(path.startswith('http')):\n",
        "    response = urlopen(path)\n",
        "    image_data = response.read()\n",
        "    image_data = BytesIO(image_data)\n",
        "    image = Image.open(image_data)\n",
        "  else:\n",
        "    image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (1, im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "ALL_MODELS = {\n",
        "'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n",
        "'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n",
        "'CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',\n",
        "'CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',\n",
        "'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n",
        "'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',\n",
        "'CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',\n",
        "'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n",
        "'CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',\n",
        "'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n",
        "'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n",
        "'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n",
        "'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n",
        "'EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1',\n",
        "'EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1',\n",
        "'EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1',\n",
        "'EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1',\n",
        "'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n",
        "'SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',\n",
        "'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n",
        "'SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',\n",
        "'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n",
        "'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n",
        "'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n",
        "'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n",
        "'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n",
        "'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',\n",
        "'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n",
        "'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n",
        "'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n",
        "'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n",
        "'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n",
        "'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n",
        "'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n",
        "'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n",
        "'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1',\n",
        "}\n",
        "\n",
        "IMAGES_FOR_TEST = {\n",
        "  'Beach' : 'models/research/object_detection/test_images/image2.jpg',\n",
        "  'Dogs' : 'models/research/object_detection/test_images/image1.jpg',\n",
        "  # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg\n",
        "  'Naxos Taverna' : 'https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg',\n",
        "  # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg\n",
        "  'Beatles' : 'https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg',\n",
        "  # By Américo Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg\n",
        "  'Phones' : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',\n",
        "  # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg\n",
        "  'Birds' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',\n",
        "}\n",
        "\n",
        "COCO17_HUMAN_POSE_KEYPOINTS = [(0, 1),\n",
        " (0, 2),\n",
        " (1, 3),\n",
        " (2, 4),\n",
        " (0, 5),\n",
        " (0, 6),\n",
        " (5, 7),\n",
        " (7, 9),\n",
        " (6, 8),\n",
        " (8, 10),\n",
        " (5, 6),\n",
        " (5, 11),\n",
        " (6, 12),\n",
        " (11, 12),\n",
        " (11, 13),\n",
        " (13, 15),\n",
        " (12, 14),\n",
        " (14, 16)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14bNk1gzh0TN"
      },
      "source": [
        "## Herramientas de visualización\n",
        "\n",
        "Para visualizar los objetos detectados con los bounding boxes y su segmentación, utilizaremos la *Object Detection API* de TensorFlow. Para instalarlo clonamos el repositorio el github."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi28cqGGFWnY"
      },
      "source": [
        "# Clone the tensorflow models repository\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX3pb_pXDjYA"
      },
      "source": [
        "Ahora instalamos la Object Detection API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwdsBdGhFanc"
      },
      "source": [
        "%%bash\n",
        "sudo apt install -y protobuf-compiler\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip --use-deprecated=legacy-resolver install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yDNgIx-kV7X"
      },
      "source": [
        "Importamos las librerías que vamos a necesitar en el notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JCeQU3fkayh"
      },
      "source": [
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKtD0IeclbL5"
      },
      "source": [
        "Y finalmente vamos a cargar las etiquetas del dataset. Las etiquetas (o labels) corresponden a números enteros que están asociados a los nombres de las categorías del dataset, de modo que cuando CNN predice `5`, sabemos que esto corresponde a `airplane`. Para esto vamos a usar algunas funciones de la API que nos ayudan a simplificar el código."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mucYUS6exUJ"
      },
      "source": [
        "PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6917xnUSlp9x"
      },
      "source": [
        "## Eligiendo el modelo de detección\n",
        "\n",
        "Ahora vamos a elegir el modelo de detección de objetos que queramos utilizar. Para esto, tenemos que seleccionar la arquitectura deseada y el modelo se cargará automáticamente. Acá estamos usando los *form* del Colab, por lo que la lista de modelos la pueden elegir de la lista desplegable de la derecha (acá les dejo el [tutorial de Colab](https://colab.research.google.com/drive/1a6y9DN3BNA8qim-1hqCqId48hUoY44Cr) donde se introducen los forms)\n",
        "\n",
        "Si se quiere cambiar el modelo para probar otras arquitecturas más tarde, simplemente se elige otra y se ejecuta la celda y todas las siguientes (recordá que lo podes hacer con un solo clik desde el menu `Entorno de Ejecución -> Ejecutar celda seleccionada y las siguientes`).\n",
        "\n",
        "**NOTA**: si desea conocer más detalles sobre el modelo seleccionado, puede seguir el enlace que se imprime en la salida (también disponible en la lista `ALL_MODELS`) y leer la documentación adicional en TF Hub.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtwrSqvakTNn"
      },
      "source": [
        "#@title Model Selection\n",
        "model_display_name = 'Faster R-CNN Inception ResNet V2 640x640' # @param ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\n",
        "model_handle = ALL_MODELS[model_display_name]\n",
        "\n",
        "print('Selected model:'+ model_display_name)\n",
        "print('Model Handle at TensorFlow Hub: {}'.format(model_handle))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muhUt-wWL582"
      },
      "source": [
        "## Cargando el modelo seleccionado\n",
        "\n",
        "Acá solo necesitamos el identificador del modelo que se seleccionó y usamos la biblioteca de Tensorflow Hub para cargarlo en la memoria.\n",
        "\n",
        "**Este proceso lleva algunos minutos** (les sobre para calentar el agua del mate o hacerse un café).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBuD07fLlcEO"
      },
      "source": [
        "print('loading model...')\n",
        "hub_model = hub.load(model_handle)\n",
        "print('model loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx7Xopat-CAe"
      },
      "source": [
        "**Opcionalmente**, se pueden cargar los modelos disponibles en TF Hub para ser usados con la libreria Keras (tal como veníamos haciendo en los notebooks anteriores). Una ventaja de esto es que podemos visualizar la arquitectura de la red mediante la función `summary()`. Dado que la siguiente celda requiere varios minutos para ejecutarse comentamos su contenido, pero si les da curiosidad mirar de cerca la arquitectura del modelo elegido no dejen de ejecutarla! (aunque seguramente requiera algo más de esfuerzo para acomodar las capas de input y output del modelo keras).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOJu89lh7HKs"
      },
      "source": [
        "#print('loading keras model...')\n",
        "#keras_model = tf.keras.Sequential([\n",
        "#              hub.KerasLayer(model_handle,\n",
        "#                              output_shape=[20],\n",
        "#                              input_shape=[]),\n",
        "#              tf.keras.layers.Dense(10,\n",
        "#                              activation='softmax')])\n",
        "#print('model loaded!')\n",
        "#\n",
        "#keras_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIawRDKPPnd4"
      },
      "source": [
        "## Cargando una imagen de testeo\n",
        "\n",
        "Dado que todos los modelos en la colección TensorFlow Hud ya han sido entrenados, podemos comenzar a clasificar nuevas imágenes son hacer nada más. Ahora probemos el modelo en una imagen simple. Para ayudar con esto vamos a usar la lista de imágenes de prueba disponible en la primera celda de código del notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atividad 4.1.3\n",
        "\n",
        "Las imagenes fueron cargadas en un repositorio github para facilitar el acceso.\n",
        "https://github.com/Odnucaf/Diplo_Datos/tree/4a3027fb0c0117228bfd56c578b525ca26e7b1be/Vision%20por%20computadora"
      ],
      "metadata": {
        "id": "ZYC1yWh_DCZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clonamos el repositorio"
      ],
      "metadata": {
        "id": "zDGWT797FyBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clona el repositorio de GitHub\n",
        "!git clone https://github.com/Odnucaf/Diplo_Datos.git\n",
        "\n",
        "# Ruta de la carpeta que contiene las imágenes\n",
        "path = \"Diplo_Datos/Vision por computadora\""
      ],
      "metadata": {
        "id": "9_qL_VGcFxoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos los nombres de los archivos para facilitar la seleccion"
      ],
      "metadata": {
        "id": "qV0dgNW0F1GN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def obtener_url_por_nombre(nombre, datos):\n",
        "    for item in datos:\n",
        "        if item[0] == nombre:\n",
        "            return item[1]\n",
        "    return None\n",
        "\n",
        "# Recorre la carpeta y lee todas las imágenes\n",
        "for filename in os.listdir(path):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
        "        # Lee la imagen\n",
        "        img = Image.open(os.path.join(path, filename))\n",
        "\n",
        "        # Crea un objeto con el mismo nombre que el archivo original\n",
        "        locals()[os.path.splitext(filename)[0]] = img\n",
        "\n",
        "file_names = []\n",
        "for filename in os.listdir(path):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
        "        # Agrega el nombre de archivo y su ruta a la lista\n",
        "        file_names.append([filename.replace('.jpg','').replace('.jpeg',''), os.path.join(path, filename)])\n",
        "\n",
        "# Crea un DataFrame con los nombres de archivo y sus rutas\n",
        "df = pd.DataFrame(file_names, columns=['Nombre de los archivos', 'Ruta'])\n",
        "\n",
        "# Imprime el DataFrame\n",
        "print(df['Nombre de los archivos'])\n"
      ],
      "metadata": {
        "id": "HktuwxryC9-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX-AWUQ1wIEr"
      },
      "source": [
        "#@title Image Selection (NO TE OLVIDES DE EJECUTAR LA CELDA!)\n",
        "selected_image = 'Birds' #@param ['Beach', 'Dogs', 'Naxos Taverna', 'Beatles', 'Phones', 'Birds']\n",
        "flip_image_horizontally = False #@param {type:\"boolean\"}\n",
        "convert_image_to_grayscale = False #@param {type:\"boolean\"}\n",
        "Utilizara_imagenes_act_4_1_3 = True #@param {type:\"boolean\"}\n",
        "seleccionar_imagen = 'Montanas02' #@param ['Monumento01', 'Montanas02', 'Disenos02', 'AnimalesMarinos01', 'Bosques01', 'Bosques02', 'Disenos01', 'Monumento02', 'Montanas01', 'Edificios02', 'Edificios01', 'AnimalesMarinos02']\n",
        "pre01_image_path = IMAGES_FOR_TEST[selected_image]\n",
        "pre02_image_path = obtener_url_por_nombre(seleccionar_imagen, file_names)\n",
        "\n",
        "if Utilizara_imagenes_act_4_1_3:\n",
        "  image_np = load_image_into_numpy_array(pre02_image_path)\n",
        "else:\n",
        "  image_np = load_image_into_numpy_array(pre01_image_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Flip horizontally\n",
        "if(flip_image_horizontally):\n",
        "  image_np[0] = np.fliplr(image_np[0]).copy()\n",
        "\n",
        "# Convert image to grayscale\n",
        "if(convert_image_to_grayscale):\n",
        "  image_np[0] = np.tile(\n",
        "    np.mean(image_np[0], 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_np[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTHsFjR6HNwb"
      },
      "source": [
        "## Detectando objetos\n",
        "\n",
        "Para hacer la inferencia solo necesitamos llamar a nuestro modelo TF Hub recien cargado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb_siXKcnnGC"
      },
      "source": [
        "\n",
        "# running inference\n",
        "results = hub_model(image_np)\n",
        "\n",
        "# different object detection models have additional results\n",
        "# all of them are explained in the documentation\n",
        "result = {key:value.numpy() for key,value in results.items()}\n",
        "print(result.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ5VYaBoeeFM"
      },
      "source": [
        "## Visualizando los resultados\n",
        "\n",
        "Acá vamos a utilizar la API de detección de objetos de TensorFlow para mostrar los bounding box de las detecciones.\n",
        "\n",
        "**TIP**: en la función `visualize_boxes_and_labels_on_image_array()` (linea 15) puede establecer valores diferentes para `min_score_thresh` y `max_boxes_to_draw` para permitir más o menos detecciones en la imagen de salida. La documentación completa de este método se puede ver [acá](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O7rV8g9s8Bz"
      },
      "source": [
        "#@title Visualizar los resultados\n",
        "\n",
        "label_id_offset = 0\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "max_boxes_to_draw=127 #@param {type:\"slider\", min:1, max:300, step:1}\n",
        "min_score_thresh=0.37 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "# Use keypoints if available in detections\n",
        "keypoints, keypoint_scores = None, None\n",
        "if 'detection_keypoints' in result:\n",
        "  keypoints = result['detection_keypoints'][0]\n",
        "  keypoint_scores = result['detection_keypoint_scores'][0]\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_detections[0],\n",
        "      result['detection_boxes'][0],\n",
        "      (result['detection_classes'][0] + label_id_offset).astype(int),\n",
        "      result['detection_scores'][0],\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=max_boxes_to_draw,\n",
        "      min_score_thresh=min_score_thresh,\n",
        "      agnostic_mode=False,\n",
        "      keypoints=keypoints,\n",
        "      keypoint_scores=keypoint_scores,\n",
        "      keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS)\n",
        "\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_np_with_detections[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ln55VWBfjgc"
      },
      "source": [
        "---\n",
        "\n",
        "# Trabajo Práctico 2 (primera parte)\n",
        "\n",
        "**Acá tienen que dejar los datos de las y los integrantes del grupo:**\n",
        "\n",
        "Facundo Melendi Suarez DNI:40835638, facundomelendi@unc.edu.ar\n",
        "\n",
        "Esequiel Armneria DNI: 35190750, esequiel1308@gmail.com\n",
        "\n",
        "Marianela Carubelli DNI: 26103422 mcarubelli@unc.edu.ar\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j3DS0bPOXJd"
      },
      "source": [
        "## **EJERCICIO 4.1**\n",
        "\n",
        "Ahora vamos a probar algunas variantes simples sobre el tutorial previamente desarrollado. Las preguntas encada intem son generativas y se deben tener en cuenta para el ejercicio 4.2.\n",
        "\n",
        "1. **Elejir el modelo de detección**. Indague en la web sobre los algoritmos del estado de arte para detección de objetos.\n",
        " - ¿Encontrás alguno de estos algoritmos en la lista de modelos disponible en TF Hub? Reportá al menos 3.\n",
        " - Seleccioná uno de los tres modelos justificando por que se eligió.\n",
        " - Volvé a ejecutar el tutorial para ese modelo\n",
        "\n",
        "2. **Modificar las imágenes de entrada**. Con este nuevo modelo, volvé a detectar objetos en las imágenes de testeo, pero ahora activá y desactivá los checkbox de la celda donde cargamos las imágenes de prueba.\n",
        " - ¿Notas diferencias en los resultados de detección entre la imagen original y la transformada? Informá cuales son estas diferencias (incluso podés mostrar imágenes con los diferentes resutaldos).\n",
        "\n",
        "3. **Ejecutar la detección en tus propias imágenes**. Ya comentamos que podés agregar todas las imágenes que quieras en la celda de selección de imágenes testeo. Buscá en la web al menos 2 imágenes de cada una de las siguientes categorías: animales marinos, figuras prediseñadas (dibujos digitales en color o blanco y negro), edificios, monumentos (pirámides, torre Eiffel, Obelisco, etc.), montañas (sin personas, sin objetos hechos por el hombre, etc.), bosques.\n",
        " - ¿Que tan bien funciona detectando objetos de estas categorias?\n",
        " - ¿Cuál crees que sea el problema de estos modelos para detectar estos objetos? (TIP: podes revisar las características del [dataset COCO 2017](https://cocodataset.org/#explore))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 1\n",
        "\n",
        "1. Faster R-CNN Inception ResNet V2\n",
        "2. SSD ResNet152 V1 FPN\n",
        "3. CenterNet HourGlass104 Keypoints\n",
        "\n",
        "\n",
        "The choice between these architectures depends on your specific use case and the trade-off between speed and accuracy.\n",
        "If you need high accuracy and are willing to tolerate slightly slower inference times, Faster R-CNN with Inception ResNet V2 might be a good choice.\n",
        "If speed is crucial and accuracy can be somewhat compromised, SSD with ResNet152 V1 FPN is a strong option. If your primary goal is keypoint estimation, CenterNet HourGlass104 Keypoints is the way to go.\n",
        "\n",
        "En base a estos argumentos, seleccionamos  Faster R-CNN with Inception ResNet V2, ya que preferimos una alta precisión y podemos tolerar tiempos de inferencia ligeramente más lentos.\n",
        "\n",
        "\n",
        "# Punto 2\n",
        "\n",
        "\n",
        "\n",
        "1.   En el caso de la imagenes de los celulares, el cambio de posicion de la imagen, tiene como consecuencia que ciertos objetos no sean detectados. En este caso la libreta azul.\n",
        "\n",
        "2.   En el caso de la imagen de la playa, la inversion de la imagen nos lleva a que el algoritmo detecte mas personas que si no la invirtieramos. En el cambio a escala de grises, no se produce ninguna diferencia.\n",
        "\n",
        "3. En el caso de la imagen de los perros, al invertir la imagen o pasarla a escala de grises, dejamos de obtener detecciones erroneas, se detectaba un perro como una vaca.\n",
        "\n",
        "4. La imagen del bar, al cambiarla a escala de grises, perdemos deteccion de algunos objetos, por ejemplos sillas, que son detectados en la imagen a color.\n",
        "\n",
        "5. La imagenes con los escarabajos, al cambiar de colores e invertir imagen, se detectan menos objetos. Si bien en la imagen en color, se detecta un objeto mas, pero esa deteccion es erronea.\n",
        "\n",
        "6. En la imagen de los pajaros, al cambiar de color, se dejan de detectar algunos objetos y al invertirla y ponerla en escala de grieses se empiezan a detectar objetos de forma incorrecta, como Book y algunos Brids que no existen.\n",
        "\n",
        "\n",
        "\n",
        "# Punto 3"
      ],
      "metadata": {
        "id": "D2QaEyG9-06N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bajamos un conjunto de imágenes de paisajes (montañas, bosques), monumentos, edificios, animales marinos y diseño, y lo cargamos en el visualizador de imágenes, para ver cómo funciona el modelo detectando objetos.\n",
        "\n",
        "Lo que se observa es que el modelo no funciona bien con este tipo de imágenes.\n",
        "\n",
        "Por ejemplo en la imágen de animales marinos, detecta algunos peces como pájaros, y algas como brócoli.\n",
        "En imágenes de paisajes, como bosques y monatañas, y de monumentos, no detecta ningún objeto. En la imagen de diseño detecta objetos como paraguas, cuchara o florero, que en realidad son partes del cuerpo humano diseñado.\n",
        "\n",
        "Esto se debe a que estas imágenes no contienen ningún objeto como los objetos con los que fueron entrenados estos modelos, por lo tanto el modelo no puede reconocer estos objetos diferentes, como árboles, montañas, peces, algas, o edificios.\n",
        "\n"
      ],
      "metadata": {
        "id": "GOFVvc5MriyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La influencia de las características de los datasets se hace evidente en los resultados obtenidos. El conjunto COCO 2017 presenta imágenes en dimensiones de 1280 x 720, mientras que las imágenes descargadas muestran una mayor resolución y en algunos casos, escalas diversas. Además, contrastan en sus enfoques: COCO 2017 exhibe escenas cotidianas, mientras que las imágenes descargadas parecen orientarse hacia propósitos artísticos. Esta distinción se manifiesta en la configuración y edición visual, desde la paleta de colores hasta los contrastes, brillos y saturaciones, así como en la disposición espacial de los objetos.\n",
        "\n",
        "La semántica de las categorías analizadas juega un papel crucial. Mientras 'Auto' agrupa cajas metálicas estilizadas, con cuatro ruedas y vidrios en todas las caras, los 'Monumentos' pueden variar en altura, color, textura y complejidad de relieve, demostrando una diversidad mucho más amplia.\n",
        "\n",
        "Una característica fundamental se destaca: los objetos etiquetados en COCO 2017 son comúnmente utilizados para entrenar modelos y aparecen en múltiples formas en las imágenes. Comparando las etiquetas 'Montaña' presente en el dataset descargado y 'Avión' en COCO 2017, se evidencia que los aviones se visualizan frecuentemente como parte del paisaje, ya sea en los cielos o en su presencia masiva."
      ],
      "metadata": {
        "id": "dWaau_qawhHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EJERCICIO 4.2**: informe y conclusiones del ejercicio 4.1"
      ],
      "metadata": {
        "id": "3M4Yyg8lishJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este trabajo practico comenzamos a evaluandar distintos algoritmos de detección de objetos disponibles en TF Hub observamos cómo estos modelos detectan objetos en diversas imágenes del dataset COCO 2017.\n",
        "\n",
        "Luego seleccionamos tres modelos y revisamos la documentacion de modelos disponibles y sus caracteristicas, decidiendo utilizar Faster R-CNN Inception ResNet V2 ya que demostró una alta precisión a pesar de tener un costo computacional mas alto.\n",
        "\n",
        "Una vez con el modelo elegido analizamos su comportamiento al detectar objetos en distintas imágenes. Modificamos las imagenes convitiendolas a blanco y negro o invertirlas horizontalmente, lo que nos permitió observar cómo variaba la detección de los objetos ante estas transformaciones.\n",
        "\n",
        "Luego descargamos imágenes de paisajes, monumentos, animales marinos y diseños para cargarlas en la notebook. Sin embargo, notamos que el modelo no funcionaba correctamente en este caso, ya que estas imágenes no contenían objetos similares a los presentes en el conjunto de datos utilizado para entrenar el modelo.\n",
        "\n",
        "Como conclusión, hemos analizado modelos preentrenados de TensorFlow Hub diseñados para la detección de objetos. Aunque su desempeño fue sólido con las imágenes iniciales, notamos variaciones notables en su capacidad de detección al realizar modificaciones en las imágenes, como el cambio de color a blanco y negro. Además, observamos que estos modelos no funcionaron adecuadamente con imágenes distintas, como paisajes. Esto resalta la importancia de analizar minuciosamente si las imágenes que se usarán son similares a las del entrenamiento y si los objetos a detectar estaban presentes en las imágenes utilizadas para el entrenamiento del modelo, ya que de lo contrario, el modelo no será efectivo."
      ],
      "metadata": {
        "id": "3OqYEBIWZ7XO"
      }
    }
  ]
}